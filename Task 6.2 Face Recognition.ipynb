{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Collect dataset by clicking your pics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:11: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:11: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-1-ad8562117391>:11: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if faces is ():\n"
     ]
    }
   ],
   "source": [
    "import cv2, numpy\n",
    "\n",
    "#Loading HAAR face classifier\n",
    "face_classifier=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Function detects faces and returns the cropped face. If no face detected, it returns the input image\n",
    "def face_extractor(img):\n",
    "    gray=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces=face_classifier.detectMultiScale(img, scaleFactor=1.3, minNeighbors=5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "        \n",
    "    # Crop all faces found\n",
    "    for (x,y,w,h) in faces:\n",
    "        cropped_face=img[y:y+h,x:x+w]\n",
    "    return cropped_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Webcam\n",
    "cap=cv2.VideoCapture(0)\n",
    "count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "face not found\n",
      "Collecting Samples Complete\n"
     ]
    }
   ],
   "source": [
    "# Collect 100 samples of your face from webcam input\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    \n",
    "    if face_extractor(frame) is not None:\n",
    "        count+=1\n",
    "        face=cv2.resize(face_extractor(frame), (200,200))\n",
    "        face=cv2.cvtColor(face,cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Save file in specified directory with unique name\n",
    "        file_name_path='./faces/user2/' + str(count) + '.jpg'\n",
    "        cv2.imwrite(file_name_path,face)\n",
    "        \n",
    "        # Put count on images and display live count\n",
    "        cv2.putText(face, str(count), (50,50), cv2.FONT_HERSHEY_COMPLEX, 1,(0,255,0), 2 )\n",
    "        cv2.imshow(\"face cropper\", face)\n",
    "        \n",
    "    else:\n",
    "        print(\"face not found\")\n",
    "        \n",
    "    if cv2.waitKey(2) == 13 or count == 100:\n",
    "        break\n",
    "        \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      \n",
    "print(\"Collecting Samples Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, numpy, os\n",
    "path=\"./faces/user2/\"\n",
    "\n",
    "# Get the training data we previously made\n",
    "data_path=path\n",
    "onlyfiles = [f for f in os.listdir(data_path) if os.path.isfile(os.path.join(data_path, f))]\n",
    "\n",
    "#print(onlyfiles)\n",
    "#print(type(onlyfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays for training data and labels\n",
    "Training_Data, Labels = [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open training images in our datapath. Create a numpy array for training data\n",
    "for i,files in enumerate(onlyfiles):\n",
    "    image_path=data_path + onlyfiles[i]\n",
    "    images = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    Training_Data.append(numpy.asarray(images,dtype=numpy.uint8))\n",
    "    Labels.append(i)\n",
    "    \n",
    "# Create a numpy array for both training data and labels\n",
    "Labels=numpy.asarray(Labels,dtype=numpy.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your name: Krithika\n"
     ]
    }
   ],
   "source": [
    "name=input(\"Enter your name: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model for \"+name+\" trained successfully\") # ignore this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Krithika trained successfully\n"
     ]
    }
   ],
   "source": [
    "# Let's train our model \n",
    "model_name=cv2.face_LBPHFaceRecognizer.create()\n",
    "model_name.train(numpy.asarray(Training_Data),numpy.asarray(Labels))\n",
    "print(\"Model \"+name+\" trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Run Our Facial Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, numpy, os\n",
    "face_classifier=cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:6: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-9-37fbe04b0d40>:6: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if faces is ():\n"
     ]
    }
   ],
   "source": [
    "def face_detector(img,size=0.5):\n",
    "    \n",
    "    # Convert image to grayscale\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces=face_classifier.detectMultiScale(gray,scaleFactor=1.3, minNeighbors=5)\n",
    "    if faces is ():\n",
    "        return img, []\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,255),2)\n",
    "        roi=img[y:y+h,x:x+w]\n",
    "        roi=cv2.resize(roi,(200,200))\n",
    "    return img, roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89, 27.661112733985615)\n"
     ]
    }
   ],
   "source": [
    "# Open Webcam\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    #print(frame)\n",
    "    image, face = face_detector(frame)\n",
    "    try:\n",
    "        face=cv2.cvtColor(face,cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Pass face to prediction model\n",
    "        # \"results\" comprises of a tuple containing the label and the confidence value\n",
    "        results=model_name.predict(face)\n",
    "        print(results)\n",
    "        \n",
    "        if results[1]<500:\n",
    "            confidence=int(100*(1-results[1]/400))\n",
    "            display_string=str(confidence)+\"% confident it is \"+name\n",
    "            \n",
    "        cv2.putText(image,display_string,(100,200),cv2.FONT_HERSHEY_COMPLEX,1, (255,120,150),2)\n",
    "        \n",
    "        if confidence > 80:\n",
    "            cv2.putText(image,\"Hey \"+name,(250,450), cv2.FONT_HERSHEY_COMPLEX,1, (0,255,0),2)\n",
    "            cv2.imshow(\"face Recognition\", image)\n",
    "            ############ CODE TO AUTOMATE: HERE ###########################\n",
    "            ######## code for aws instance launch using terraform #############\n",
    "            # Launching aws instance, volume; and attaching this vol with instance using terraform\n",
    "                    \n",
    "            #running the terraform code\n",
    "            \n",
    "            #os.system(r\"cd 'C:\\Users\\kritika\\Desktop\\MytfTask6\\'\")\n",
    "            os.system(\"terraform init\") #downloading the plugins\n",
    "            os.system(\"terraform apply --auto-approve\")\n",
    "            break\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            cv2.putText(image,\"I don't know who r u\",(250,450),cv2.FONT_HERSHEY_COMPLEX,1,(0,0,255),2)\n",
    "            cv2.imshow(\"Face Recognition\", image)\n",
    "            \n",
    "    except:\n",
    "        cv2.putText(image, \"No Face Found\", (220, 120) , cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "        cv2.putText(image, \"looking for face\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "        cv2.imshow('Face Recognition', image )\n",
    "        pass\n",
    "        if cv2.waitKey(20) == 13: #13 is the Enter Key\n",
    "            break\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
